# Backpropagation
Okay so, I'm sure theoretically, gradient descent makes sense to you. We find the gradient (the derivative) of each neuron with respect to the loss, and change it such that
