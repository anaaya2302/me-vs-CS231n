# The most uneventful note of the lot
This is mostly facts... Nothing too crazy's gonna happen here. But... I'm sure you'll appreciate the way it's been put.

## The 50s
- **Hubel and Wiesel - 1959**: This is where everything started. Hubel and Wiesel did a study on the cat brain, the specifics of which are irrelevant for our purposes, and tried to understand how visual processing works.  The primary visual cortex of the brain contained three types of cells.  
  Simple cells: Respond to orientation of light i.e edges.   
  Complex cells: Respond to orientation _and_ movement.  
  Hypercomplex cells: Respond to movement with an end point. <- yes, ik it's vague but it's enough for our purposes -> ( now called end stopped cells, honestly this rebrand is as bad as Jaguar's )  
  Point being: Visual processing starts with basic edges
## The 60s
- **Larry Roberts - 1963**: Reconstructed geometric shapes using edges.
- **MIT - 1966**: This was a summer project that tried to solve vision. <- I'm not joking, ONE summer ( They failed at that but learnt a lot so... screw around and find out, ig ) ->
## The 70s 
- **David Marr - 1970**: wrote a book on vision. Proposed an algorithm that started with edge detection, pieced together surfaces and layers and then reconstructed a 3d model. <- Philosophy being: Everything can be represented with very simple geometric shapes. Valid if you think about it... That's what artists do on a preliminary sketch->
- **Seppo Linnainmaa - 1970**: The very same year, The modern version of backprapogation was created. <-Controversial take: Werbos applied it to neural nets in 1981, but no one listened. Rumelhart and Hinton saw the potential and got it to the mainstream. They were the kids that said your joke, just louder and the whole class laughed ->
## The 80s
- **Kunihiko Fukushima - 1980** : The Neocognitron. Like a CNN, but without backprop and with hardcoded layers.
- This marked the beginning of the 'ai winter'. Basically people lost hope in the field, but it came back stronger than a 90s trend.
## The 90s
- **1997** : Everyone went nvm this is too hard... Let's start with something called segmentation. This was sorta like attention... emphasis on sorta. It divided images into segments using graph theory algorithms.
- **Yann LeCun - 1998**: The CNN. This was really genius... And worked pretty well for digit recognition, but there wasn't enough data or processing power at the time.
- **David Lowe - 1999**: SIFT. Scale invariant feature transform. I know that sounds like fancy mumbo jumbo. But it's essentially saying, there are some features of every object, that stay virtually the same no matter the orientation, lighting or deformation of the object. It was onto something, and did okay for stiff objects.
## Stuff gets interesting  
There was like a crazy improvement in image quality. I'm too young to attest to that so I'm taking the megapixels at their word. Also, computational power went crazy. But, our architectures stayed pretty much similar to previous ones until transformers spawned in 2017. 
- **Adaboost, 2001**: A facial detection program, In real time. And just five years later fujifilm made a camera with the tech.
- **HOG and SVM, 2005**: Histogram of Oriented Gradients and a Support Vector Machine. It's a fancy algorithm that's really good at going True or False. (Is this a cat? That's it). Doesn't generalise but is really good at its job. Was initially used to detect pedestrians.
- **PASCAL**: A benchmark challenge to see how far our visual systems have gotten.
- **Imagenet and Wordnet**: These were where things got crazy. These were huge datasets, properly labeled, sorted and cleaned. 15 million images. <- No shit people labelled them by hand. INTO 22,000 CATEGORIES. Shout out to Amazon Mechanical Turk Workers ->
- **2012**: Alexnet... The modern CNN. It was... a god. It dropped the error rate from 25 percent on the Imagenet challenge to 16 percent. IN ONE YEAR.
- After this... A lot happens... LIKE A LOT. so that'll go along with the notes. But even upto this point... We were throwing huge amounts of data at ridiculously large functions and hoping for underlying patterns... yep.

 

  
  
