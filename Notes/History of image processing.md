# The most uneventful note of the lot
This is mostly facts... Nothing too crazy's gonna happen here. But... I'm sure you'll appreciate the way it's been put.

## The 50s
- **Hubel and Wiesel - 1959**: This is where everything started. Hubel and Weisel did a study on the cat brain, the specifics of which are irrelevant for our purposes, and tried to understand how visual processing works.  The primary visual cortex of the brain contained three types of cells.  
  Simple cells: Respond to orientation of light i.e egdes.   
  Complex cells: Respond to orientation _and_ movement.  
  Hypercomplex cells: Respond to movement with an end point. <- yes, ik it's vague but it's enough for our purposes ->  
  Point being: Visual processing starts with basic edges
## The 60s
- **Larry Roberts - 1963**: Reconstruted geometric shapes using edges.
- **MIT - 1966**: This was a summer project that tried to solve vision. <- I'm not joking, ONE summer ( They failed at that but learnt a lot so... fuck around and find out, ig ) ->
## The 70s 
- **David Marr - 1970**: wrote a book on vision. Proposed an algorithm that started with edge detection, pieced together surfaces and layers and then reconstructed a 3d model. <- Philosophy being: Everything can be represented with very simple geometric shapes. Valid if you think about it... That's what artists do on a preliminary sketch-> 
## The 80s
- **Kunihiko Fukushima - 1980** : The Neocognitron. Like a CNN, but unsupervised. 
## The 90s
- **1997** : Everyone went nvm this is too hard... Let's start with something called segmentation. This was sorta like attention... emphasis on sorta. It divided images into segments using graph theory algorithms.
- **Yan Le Cun - 1998**: The CNN. This was really genius... And worked pretty well for digit recognition, but there wasn't enough data or processing power at the time.
- **David Lowe - 1999**: SIFT. Scale invariant feature transform. I know that sounds like fancy mumbo jumbo. But it's essentially saying, there are some features of every object, that stay virtually the same no matter the orientation, lighting or deformation of the object. It was onto something, and did okay for stiff objects.
## Stuff gets interesting  
There was like a crazy improvement in image quality. I'm too young to attest to that so I'm taking the megapixels word for it.
- **Adaboost, 2001**: A facial detection progeam, In real time. fr. And just five years later fujifilm made a camera with the tech
- **PASCAL**: A benchmark challenge to see how far our visual systems have gotten
- **Imagenet and Wordnet**: These were where things got crazy. These were huge datasets, properly labeled, sorted and cleaned. 15 million imgaes. <- No shit people labelled them by hand. INTO 22,000 CATEGOREIS ->
- **2012**: Alexnet... The modern CNN. It was... a god. It dropped the error rate from 25 percent on the Imagenet challenge to 16 percent. IN ONE YEAR.
- After this... A lot happens... LIKE A LOT. so that'll go along with the notes. But even upto this point... We were throwing huge amounts of data at ridiculously large functions and hoping for underlying patterns... yep.

 

  
  
